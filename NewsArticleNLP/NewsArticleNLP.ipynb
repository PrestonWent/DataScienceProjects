{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment L10: Topic Identification with an ANN\n",
    "\n",
    "**Author: Preston Went**  \n",
    "**Course: DATASCI 420**  \n",
    "**DATE (MM/DD/YYYY): 03/15/2021**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#Importing-and-Preparing-the-Data\" data-toc-modified-id=\"Importing-and-Preparing-the-Data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Importing and Preparing the Data</a></span></li><li><span><a href=\"#The-Initial-Model\" data-toc-modified-id=\"The-Initial-Model-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>The Initial Model</a></span></li><li><span><a href=\"#Improving-the-Model\" data-toc-modified-id=\"Improving-the-Model-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Improving the Model</a></span></li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Conclusion</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this assignment, I will use an artificial neural network (ANN) to identify the topics of given news articles.\n",
    "\n",
    "As always, I will import all necessary libraries before starting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T03:29:45.700849Z",
     "start_time": "2021-03-16T03:29:45.691141Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns; sns.set();\n",
    "\n",
    "# Modeling\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Data preprocessing\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Model evaluation\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Set our random seed\n",
    "RAND_SEED = 5743829\n",
    "np.random.seed(RAND_SEED)\n",
    "\n",
    "# Filter some unavoidable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and Preparing the Data\n",
    "\n",
    "We will be identifying news articles on the Reuters newswire dataset, which contains 11,228 news articles across 46 topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T03:29:46.618235Z",
     "start_time": "2021-03-16T03:29:46.031400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the Reuters newswire dataset\n",
    "vocab_size = 15000\n",
    "data = keras.datasets.reuters\n",
    "(X_train, y_train), (X_test, y_test) = data.load_data(num_words=vocab_size,\n",
    "                                                      seed=RAND_SEED)\n",
    "\n",
    "# Propely encode the label vectors\n",
    "num_topics = 46\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "# Set maximum length of newswires\n",
    "max_wire_len = 600\n",
    "X_train = keras.preprocessing.sequence.pad_sequences(X_train, maxlen=max_wire_len)\n",
    "X_test = keras.preprocessing.sequence.pad_sequences(X_test, maxlen=max_wire_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T00:11:20.589216Z",
     "start_time": "2021-03-16T00:11:20.582802Z"
    }
   },
   "source": [
    "## The Initial Model\n",
    "\n",
    "We will start with a simple recurrent neural network (RNN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T03:29:48.967117Z",
     "start_time": "2021-03-16T03:29:46.619690Z"
    }
   },
   "outputs": [],
   "source": [
    "# Construct the RNN\n",
    "embed_dim = 64\n",
    "rnn_mdl = keras.Sequential([\n",
    "    keras.layers.Embedding(input_dim=vocab_size,\n",
    "                           output_dim=embed_dim,\n",
    "                           input_length=max_wire_len),\n",
    "    keras.layers.SimpleRNN(50),\n",
    "    keras.layers.Dense(num_topics,\n",
    "                       activation=tf.nn.sigmoid)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train our model on the training set. We will use the binary cross entropy as our loss function, and ADAM as our training method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T02:22:21.819310Z",
     "start_time": "2021-03-16T02:22:21.785583Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the training and evaluation process\n",
    "rnn_mdl.compile(optimizer='adam',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T02:26:22.455220Z",
     "start_time": "2021-03-16T02:22:22.181867Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8982 samples\n",
      "Epoch 1/3\n",
      "8982/8982 [==============================] - 78s 9ms/sample - loss: 2.6744 - accuracy: 0.2924\n",
      "Epoch 2/3\n",
      "8982/8982 [==============================] - 80s 9ms/sample - loss: 2.4182 - accuracy: 0.3518\n",
      "Epoch 3/3\n",
      "8982/8982 [==============================] - 82s 9ms/sample - loss: 2.4161 - accuracy: 0.3518\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c8636d9108>"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the neural network\n",
    "rnn_mdl.fit(X_train, y_train, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss and training accuracy improved very little. Let us see how the model performs on our testing set using the average precision, AUC-ROC, and classification matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T02:27:11.486264Z",
     "start_time": "2021-03-16T02:27:08.231225Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. Precision: 0.03\n",
      "AUC-ROC: 0.57\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.00      0.00        12\n",
      "           1       1.00      0.00      0.00       121\n",
      "           2       1.00      0.00      0.00        21\n",
      "           3       0.36      1.00      0.53       812\n",
      "           4       1.00      0.00      0.00       458\n",
      "           5       1.00      0.00      0.00         4\n",
      "           6       1.00      0.00      0.00         7\n",
      "           7       1.00      0.00      0.00         6\n",
      "           8       1.00      0.00      0.00        38\n",
      "           9       1.00      0.00      0.00        21\n",
      "          10       1.00      0.00      0.00        34\n",
      "          11       1.00      0.00      0.00        96\n",
      "          12       1.00      0.00      0.00        10\n",
      "          13       1.00      0.00      0.00        35\n",
      "          14       1.00      0.00      0.00         9\n",
      "          15       1.00      0.00      0.00         4\n",
      "          16       1.00      0.00      0.00       114\n",
      "          17       1.00      0.00      0.00         5\n",
      "          18       1.00      0.00      0.00        19\n",
      "          19       1.00      0.00      0.00       146\n",
      "          20       1.00      0.00      0.00        67\n",
      "          21       1.00      0.00      0.00        26\n",
      "          22       1.00      0.00      0.00         7\n",
      "          23       1.00      0.00      0.00         9\n",
      "          24       1.00      0.00      0.00        18\n",
      "          25       1.00      0.00      0.00        21\n",
      "          26       1.00      0.00      0.00         6\n",
      "          27       1.00      0.00      0.00         2\n",
      "          28       1.00      0.00      0.00         7\n",
      "          29       1.00      0.00      0.00         3\n",
      "          30       1.00      0.00      0.00        13\n",
      "          31       1.00      0.00      0.00        12\n",
      "          32       1.00      0.00      0.00         7\n",
      "          33       1.00      0.00      0.00         3\n",
      "          34       1.00      0.00      0.00        11\n",
      "          35       1.00      0.00      0.00         4\n",
      "          36       1.00      0.00      0.00         7\n",
      "          37       1.00      0.00      0.00         8\n",
      "          38       1.00      0.00      0.00         4\n",
      "          39       1.00      0.00      0.00         2\n",
      "          40       1.00      0.00      0.00         8\n",
      "          41       1.00      0.00      0.00         6\n",
      "          42       1.00      0.00      0.00         5\n",
      "          43       1.00      0.00      0.00         6\n",
      "          44       1.00      0.00      0.00         5\n",
      "          45       1.00      0.00      0.00         7\n",
      "\n",
      "   micro avg       0.36      0.36      0.36      2246\n",
      "   macro avg       0.99      0.02      0.01      2246\n",
      "weighted avg       0.77      0.36      0.19      2246\n",
      " samples avg       0.36      0.36      0.36      2246\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get predictions on the testing set\n",
    "y_pred_proba = rnn_mdl.predict(X_test)\n",
    "y_pred = np.zeros_like(y_pred_proba)\n",
    "y_pred[np.arange(len(y_pred_proba)), y_pred_proba.argmax(1)] = 1\n",
    "\n",
    "# Get and print the evaluation metrics\n",
    "print('Avg. Precision: {:.2f}'.format(average_precision_score(y_test, y_pred_proba)))\n",
    "print('AUC-ROC: {:.2f}'.format(roc_auc_score(y_test, y_pred_proba)))\n",
    "print(classification_report(y_test, y_pred, zero_division=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T00:50:09.490389Z",
     "start_time": "2021-03-16T00:50:09.485960Z"
    }
   },
   "source": [
    "The model performs quite poorly, if still better than random guesswork. It appears to all but ignore the existence of most kinds of article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving the Model\n",
    "\n",
    "Let us try to improve the model. I will do this by using Long Short-Term Memory (LSTM) instead of an RNN and an additional dense hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T02:12:51.966552Z",
     "start_time": "2021-03-16T02:12:51.747129Z"
    }
   },
   "outputs": [],
   "source": [
    "# Construct the RNN\n",
    "embed_dim = 256\n",
    "lstm_mdl = keras.Sequential([\n",
    "    keras.layers.Embedding(input_dim=vocab_size,\n",
    "                           output_dim=embed_dim,\n",
    "                           input_length=max_wire_len),\n",
    "    keras.layers.LSTM(128),\n",
    "    keras.layers.Dense(64,\n",
    "                      activation=tf.nn.relu),\n",
    "    keras.layers.Dense(num_topics,\n",
    "                       activation=tf.nn.sigmoid)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training process will move faster with this LSTM than with the RNN, so we can train for more epochs in the same amount of time. Let us do so now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T02:12:52.300011Z",
     "start_time": "2021-03-16T02:12:52.268329Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the training and evaluation process\n",
    "lstm_mdl.compile(optimizer='adam',\n",
    "                 loss='categorical_crossentropy',\n",
    "                 metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T02:20:00.468385Z",
     "start_time": "2021-03-16T02:12:52.813528Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8982 samples\n",
      "Epoch 1/20\n",
      "8982/8982 [==============================] - 22s 3ms/sample - loss: 2.5765 - accuracy: 0.3501\n",
      "Epoch 2/20\n",
      "8982/8982 [==============================] - 21s 2ms/sample - loss: 2.1377 - accuracy: 0.3977\n",
      "Epoch 3/20\n",
      "8982/8982 [==============================] - 21s 2ms/sample - loss: 1.7132 - accuracy: 0.5588\n",
      "Epoch 4/20\n",
      "8982/8982 [==============================] - 21s 2ms/sample - loss: 1.5823 - accuracy: 0.5886\n",
      "Epoch 5/20\n",
      "8982/8982 [==============================] - 21s 2ms/sample - loss: 1.8370 - accuracy: 0.5225\n",
      "Epoch 6/20\n",
      "8982/8982 [==============================] - 21s 2ms/sample - loss: 1.4445 - accuracy: 0.6004\n",
      "Epoch 7/20\n",
      "8982/8982 [==============================] - 21s 2ms/sample - loss: 1.3530 - accuracy: 0.6253\n",
      "Epoch 8/20\n",
      "8982/8982 [==============================] - 21s 2ms/sample - loss: 1.2800 - accuracy: 0.6432\n",
      "Epoch 9/20\n",
      "8982/8982 [==============================] - 21s 2ms/sample - loss: 1.1242 - accuracy: 0.6904\n",
      "Epoch 10/20\n",
      "8982/8982 [==============================] - 21s 2ms/sample - loss: 0.9108 - accuracy: 0.7524\n",
      "Epoch 11/20\n",
      "8982/8982 [==============================] - 21s 2ms/sample - loss: 0.7453 - accuracy: 0.8081\n",
      "Epoch 12/20\n",
      "8982/8982 [==============================] - 22s 2ms/sample - loss: 0.6173 - accuracy: 0.8376\n",
      "Epoch 13/20\n",
      "8982/8982 [==============================] - 21s 2ms/sample - loss: 0.5070 - accuracy: 0.8710\n",
      "Epoch 14/20\n",
      "8982/8982 [==============================] - 21s 2ms/sample - loss: 0.4203 - accuracy: 0.8932\n",
      "Epoch 15/20\n",
      "8982/8982 [==============================] - 21s 2ms/sample - loss: 0.3474 - accuracy: 0.9146\n",
      "Epoch 16/20\n",
      "8982/8982 [==============================] - 21s 2ms/sample - loss: 0.2974 - accuracy: 0.9267\n",
      "Epoch 17/20\n",
      "8982/8982 [==============================] - 21s 2ms/sample - loss: 0.2519 - accuracy: 0.9398\n",
      "Epoch 18/20\n",
      "8982/8982 [==============================] - 21s 2ms/sample - loss: 0.2272 - accuracy: 0.9419\n",
      "Epoch 19/20\n",
      "8982/8982 [==============================] - 21s 2ms/sample - loss: 0.2008 - accuracy: 0.9459\n",
      "Epoch 20/20\n",
      "8982/8982 [==============================] - 21s 2ms/sample - loss: 0.1849 - accuracy: 0.9497\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c85f2e32c8>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the neural network\n",
    "lstm_mdl.fit(X_train, y_train, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss and training accuracy improved dramatically, both from the start of training and the previous iteration of the ANN. Let us see if this is is reflected in our evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T02:20:07.735434Z",
     "start_time": "2021-03-16T02:20:05.665859Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. Precision: 0.28\n",
      "AUC-ROC: 0.84\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.58      0.61        12\n",
      "           1       0.66      0.75      0.70       121\n",
      "           2       0.77      0.48      0.59        21\n",
      "           3       0.92      0.92      0.92       812\n",
      "           4       0.83      0.88      0.85       458\n",
      "           5       0.00      0.00      0.00         4\n",
      "           6       0.60      0.43      0.50         7\n",
      "           7       0.33      0.17      0.22         6\n",
      "           8       0.35      0.50      0.41        38\n",
      "           9       0.50      0.86      0.63        21\n",
      "          10       0.77      0.71      0.74        34\n",
      "          11       0.72      0.65      0.68        96\n",
      "          12       0.14      0.20      0.17        10\n",
      "          13       0.57      0.66      0.61        35\n",
      "          14       0.25      0.11      0.15         9\n",
      "          15       0.00      0.00      0.00         4\n",
      "          16       0.65      0.56      0.60       114\n",
      "          17       0.07      0.20      0.10         5\n",
      "          18       0.60      0.32      0.41        19\n",
      "          19       0.61      0.64      0.62       146\n",
      "          20       0.60      0.40      0.48        67\n",
      "          21       0.52      0.62      0.56        26\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.29      0.56      0.38         9\n",
      "          24       0.69      0.50      0.58        18\n",
      "          25       0.62      0.71      0.67        21\n",
      "          26       0.80      0.67      0.73         6\n",
      "          27       0.00      0.00      0.00         2\n",
      "          28       0.17      0.29      0.21         7\n",
      "          29       0.00      0.00      0.00         3\n",
      "          30       0.29      0.15      0.20        13\n",
      "          31       0.33      0.17      0.22        12\n",
      "          32       0.60      0.43      0.50         7\n",
      "          33       0.50      0.33      0.40         3\n",
      "          34       0.75      0.27      0.40        11\n",
      "          35       0.00      0.00      0.00         4\n",
      "          36       0.31      0.57      0.40         7\n",
      "          37       0.00      0.00      0.00         8\n",
      "          38       0.50      0.25      0.33         4\n",
      "          39       0.00      0.00      0.00         2\n",
      "          40       0.20      0.12      0.15         8\n",
      "          41       0.33      0.17      0.22         6\n",
      "          42       1.00      0.00      0.00         5\n",
      "          43       1.00      0.33      0.50         6\n",
      "          44       0.75      0.60      0.67         5\n",
      "          45       0.75      0.43      0.55         7\n",
      "\n",
      "   micro avg       0.75      0.75      0.75      2246\n",
      "   macro avg       0.46      0.37      0.38      2246\n",
      "weighted avg       0.75      0.75      0.74      2246\n",
      " samples avg       0.75      0.75      0.75      2246\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get predictions on the testing set\n",
    "y_pred_proba = lstm_mdl.predict(X_test)\n",
    "y_pred = np.zeros_like(y_pred_proba)\n",
    "y_pred[np.arange(len(y_pred_proba)), y_pred_proba.argmax(1)] = 1\n",
    "\n",
    "# Get and print the evaluation metrics\n",
    "print('Avg. Precision: {:.2f}'.format(average_precision_score(y_test, y_pred_proba)))\n",
    "print('AUC-ROC: {:.2f}'.format(roc_auc_score(y_test, y_pred_proba)))\n",
    "print(classification_report(y_test, y_pred, zero_division=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LSTM is a dramatic improvement over the RNN. It still has problems identifying the less common topics, but not nearly so much as the RNN. The order of magnitude increase in average precision is indicative of this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We constructed two ANNs - a RNN and a LSTM - to classify the topic of news articles, training and testing them on the Reuters newswire dataset. The LSTM dramatically outperformed the RNN - particularly on the smaller classes. The challenge with those appears to be not having enough examples to train on. We cannot resample them without causing overfitting, and oversampling is difficult with such complex data. I plan on looking into this problem more later."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
